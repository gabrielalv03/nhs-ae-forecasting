library(dplyr)
library(tidyr)
library(ggplot2)
library(stringr)
library(DT)
library(htmlwidgets)
library(writexl)

# 1. Error Metrics ---------------------------------------------------------

rmse <- function(actual, predicted) sqrt(mean((actual - predicted)^2, na.rm = TRUE))
mae  <- function(actual, predicted) mean(abs(actual - predicted), na.rm = TRUE)
r_squared <- function(actual, predicted) {
  ss_res <- sum((actual - predicted)^2, na.rm = TRUE)
  ss_tot <- sum((actual - mean(actual, na.rm = TRUE))^2, na.rm = TRUE)
  return(1 - ss_res / ss_tot)
}
percent_rmse <- function(actual, predicted) {
  rmse(actual, predicted) / mean(actual, na.rm = TRUE) * 100
}
percent_mae <- function(actual, predicted) {
  mae(actual, predicted) / mean(actual, na.rm = TRUE) * 100
}

# 2. Metric Computation Functions ------------------------------------------

calculate_metrics <- function(df, actual_col, pred_prefix) {
  model_cols <- grep(paste0("^", pred_prefix), names(df), value = TRUE)
  lapply(model_cols, function(col) {
    pred <- df[[col]]
    actual <- df[[actual_col]]
    data.frame(
      Model = gsub(pred_prefix, "", col),
      RMSE = rmse(actual, pred),
      MAE = mae(actual, pred),
      pct_RMSE = percent_rmse(actual, pred),
      pct_MAE = percent_mae(actual, pred),
      R2 = r_squared(actual, pred),
      Residual_Mean = mean(actual - pred, na.rm = TRUE),
      Residual_SD = sd(actual - pred, na.rm = TRUE)
    )
  }) %>% bind_rows()
}

# 3. Task-specific Comparison -----------------------------------------------

evaluate_task <- function(df, actual_col, pred_prefix, task_label) {
  metrics <- calculate_metrics(df, actual_col, pred_prefix)
  metrics$Task <- task_label
  
  metrics <- metrics %>%
    group_by(Task) %>%
    mutate(
      Rank_pct_RMSE = rank(pct_RMSE, ties.method = "min"),
      Rank_pct_MAE  = rank(pct_MAE,  ties.method = "min"),
      Stability_Rank = Rank_pct_RMSE + Rank_pct_MAE
    ) %>%
    ungroup() %>%
    group_by(Task) %>%
    mutate(
      Best_Model = ifelse(Stability_Rank == min(Stability_Rank), "âœ…", "")
    ) %>%
    ungroup()
  
  return(metrics)
}

# 4. Run For All Tasks ------------------------------------------------------

df1 <- evaluate_task(comparison_att_region_df,   "actual_AE_att_tot",   "pred_", "Attendance by Region")
df2 <- evaluate_task(comparison_att_trust_df,    "actual_AE_att_tot",   "pred_", "Attendance by Trust")
df3 <- evaluate_task(comparison_over4_region_df, "actual_AE_over4_pct", "pred_", "% Over 4h by Region")
df4 <- evaluate_task(comparison_over4_trust_df,  "actual_AE_over4_pct", "pred_", "% Over 4h by Trust")

all_model_metrics_full <- bind_rows(df1, df2, df3, df4)

# Add interpretability
interpretability_levels <- tibble::tibble(
  Model = c("lm", "prophet", "rf", "xgb"),
  Interpretability = c("High", "Medium", "Low", "Low")
)

all_model_metrics_full <- all_model_metrics_full %>%
  mutate(Model_clean = str_remove(Model, "_AE_att_tot|_AE_over4_pct")) %>%
  left_join(interpretability_levels, by = c("Model_clean" = "Model")) %>%
  select(-Model_clean)

all_model_metrics_full <- all_model_metrics_full %>%
  left_join(interpretability_levels, by = "Model") %>%
  relocate(Task, Model)

# View and Save
print(all_model_metrics_full)
write.csv(all_model_metrics_full, "all_model_metrics_taskwise.csv", row.names = FALSE)

# 5. Optional: Color-Coded HTML Table --------------------------------------

model_table <- datatable(
  all_model_metrics_full,
  options = list(pageLength = 20, scrollX = TRUE),
  rownames = FALSE,
  caption = "ðŸ“Š Model Evaluation Summary (Task-Specific Ranking)"
) %>%
  
  # Color code %RMSE: lower is better
  formatStyle("pct_RMSE",
              backgroundColor = styleInterval(
                c(5, 10, 20), c('#2ecc71', '#f1c40f', '#e67e22', '#e74c3c')
              )) %>%
  
  # Color code %MAE: lower is better
  formatStyle("pct_MAE",
              backgroundColor = styleInterval(
                c(5, 10, 20), c('#2ecc71', '#f1c40f', '#e67e22', '#e74c3c')
              )) %>%
  
  # Color code RÂ²: higher is better
  formatStyle("R2",
              backgroundColor = styleInterval(
                c(0, 0.3, 0.6), c('#e74c3c', '#e67e22', '#f1c40f', '#2ecc71')
              )) %>%
  
  # Color code Stability_Rank: lower is better
  formatStyle("Stability_Rank",
              backgroundColor = styleInterval(
                c(2, 4, 6), c('#2ecc71', '#f1c40f', '#e67e22', '#e74c3c')
              )) %>%
  
  # Residual_Mean (absolute): smaller is better
  formatStyle("Residual_Mean",
              backgroundColor = styleInterval(
                c(0.01, 0.03, 0.05), c('#2ecc71', '#f1c40f', '#e67e22', '#e74c3c')
              )) %>%
  
  # Residual_SD: smaller is better
  formatStyle("Residual_SD",
              backgroundColor = styleInterval(
                c(1000, 5000, 10000), c('#2ecc71', '#f1c40f', '#e67e22', '#e74c3c')
              ))


saveWidget(model_table, file = "model_evaluation_table_taskwise.html", selfcontained = TRUE)
write_xlsx(all_model_metrics_full, "model_evaluation_taskwise.xlsx")
